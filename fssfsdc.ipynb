{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Semantic Segmentation for Self Driving Cars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: SETUP PHASE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision\n",
    "import logging\n",
    "import warnings\n",
    "import math\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.backends import cudnn\n",
    "from torch import from_numpy\n",
    "from PIL import Image\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.io import read_image # importare solo se si usa nella classe Dataset\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Configuration Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "PARTITION = 'A' # A or B to choose which dataloader to use\n",
    "SEED = 42\n",
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 19\n",
    "if NUM_CLASSES == 19:\n",
    "  cl19 = True\n",
    "\n",
    "BATCH_SIZE = 8     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing ***\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = 0.05           # The initial Learning Rate ***\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD ***\n",
    "WEIGHT_DECAY =0.0005  # Regularization, you can keep this at the default ***\n",
    "\n",
    "NUM_EPOCHS = 10     # Total number of training epochs (iterations over dataset)\n",
    "\n",
    "# servono per decrementare il lerning rate nel tempo \n",
    "STEP_SIZE = 500       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.8          # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = 20\n",
    "LOG_FREQUENCY_EPOCH = 3\n",
    "\n",
    "ROOT_DIR = os.path.join('data', 'Cityscapes')\n",
    "ROOT_DIR_GTA5 = os.path.join('data', 'GTA5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter configuration Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION = \"B\"  # 'A' or 'B'\n",
    "SPLIT = 1  # 1 or 2 // 1 = Uniform : 2 = Heterogenous\n",
    "MAX_SAMPLE_PER_CLIENT = 20\n",
    "\n",
    "IMAGES_FINAL = \"leftImg8bit\"\n",
    "TARGET_FINAL = \"gtFine_labelIds\"\n",
    "\n",
    "N_ROUND = 50\n",
    "CLIENT_PER_ROUND = 5  # clients picked each round\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "\n",
    "CHECKPOINTS = 5\n",
    "\n",
    "\n",
    "if PARTITION == 'A':\n",
    "  if SPLIT == 1:\n",
    "    TOT_CLIENT = 36\n",
    "  else:\n",
    "    TOT_CLIENT = 46\n",
    "else:\n",
    "  if SPLIT == 1:\n",
    "    TOT_CLIENT = 25\n",
    "  else:\n",
    "    TOT_CLIENT = 33\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation options\n",
    "# set as None if a transformation should not be used\n",
    "\n",
    "RANDOM_HORIZONTAL_FLIP = 0.5\n",
    "#RANDOM_HORIZONTAL_FLIP = None      #probability of the image being flipped\n",
    "COLOR_JITTER = (0.2,0.3,0.2,0.2) # (brighteness, contrast, saturation, hue)\n",
    "#COLOR_JITTER = None\n",
    "RANDOM_ROTATION = 5              # degree of rotation\n",
    "#RANDOM_ROTATION = None\n",
    "#RANDOM_CROP = (512,1024)         # output size of the crop\n",
    "RANDOM_CROP = None\n",
    "#RESIZE = (512,1024)              # output size\n",
    "RESIZE = None\n",
    "#RANDOM_VERTICAL_FLIP  = 0.3     # probability of the image being flipped\n",
    "RANDOM_VERTICAL_FLIP = None\n",
    "CENTRAL_CROP = (512,1024)\n",
    "#CENTRAL_CROP = (512,1024)\n",
    "#RANDOM_RESIZE_CROP = (1024,2048)\n",
    "RANDOM_RESIZE_CROP = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_transform():\n",
    "    transformers = []\n",
    "    if RANDOM_HORIZONTAL_FLIP is not None:\n",
    "        transformers.append(T.RandomHorizontalFlip(RANDOM_HORIZONTAL_FLIP))\n",
    "    if COLOR_JITTER is not None:\n",
    "        transformers.append(T.ColorJitter(*COLOR_JITTER))\n",
    "    if RANDOM_ROTATION is not None:\n",
    "        transformers.append(T.RandomRotation(RANDOM_ROTATION))\n",
    "    if RANDOM_CROP is not None:\n",
    "        transformers.append(T.RandomCrop(RANDOM_CROP))\n",
    "    if RANDOM_VERTICAL_FLIP is not None:\n",
    "        transformers.append(T.RandomVerticalFlip(RANDOM_VERTICAL_FLIP))\n",
    "    if CENTRAL_CROP is not None:\n",
    "        transformers.append(T.CenterCrop(CENTRAL_CROP))\n",
    "    if RANDOM_RESIZE_CROP is not None:\n",
    "        transformers.append(T.RandomResizedCrop(RANDOM_RESIZE_CROP))\n",
    "    if RESIZE is not None:\n",
    "        transformers.append(T.Resize(RESIZE))\n",
    "\n",
    "    transforms = T.Compose(transformers)\n",
    "\n",
    "    return transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cityscapes(torch_data.Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    image path: data/Cityscapes/images/name_leftImg8bit.png\n",
    "    taget path: data/Cityscapes/labels/name_gtFine_labelIds.png\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transform=None, cl19=False, filename=None, id_client=None):\n",
    "        eval_classes = [\n",
    "            7,\n",
    "            8,\n",
    "            11,\n",
    "            12,\n",
    "            13,\n",
    "            17,\n",
    "            19,\n",
    "            20,\n",
    "            21,\n",
    "            22,\n",
    "            23,\n",
    "            24,\n",
    "            25,\n",
    "            26,\n",
    "            27,\n",
    "            28,\n",
    "            31,\n",
    "            32,\n",
    "            33,\n",
    "        ]\n",
    "        self.root = root\n",
    "\n",
    "        if filename is None:\n",
    "            raise ValueError(\"filename is None\")\n",
    "\n",
    "        if id_client is not None:\n",
    "            with open(os.path.join(root, filename)) as f:\n",
    "                dict_data = json.load(f)\n",
    "\n",
    "            self.paths_images = [l[0] for l in dict_data[str(id_client)]]\n",
    "            self.paths_tagets = [l[1] for l in dict_data[str(id_client)]]\n",
    "\n",
    "            self.len = len(self.paths_images)\n",
    "            self.transform = transform\n",
    "            self.return_unprocessed_image = False\n",
    "\n",
    "        else:\n",
    "            with open(os.path.join(root, filename), \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            # manipulate each file row in order to obtain the correct path\n",
    "            self.paths_images = [l.strip().split(\"@\")[0] for l in lines]\n",
    "            self.paths_tagets = [l.strip().split(\"@\")[1] for l in lines]\n",
    "\n",
    "            self.len = len(self.paths_images)\n",
    "            self.transform = transform\n",
    "\n",
    "        if cl19:\n",
    "            classes = eval_classes\n",
    "            mapping = np.zeros((256,), dtype=np.int64) + 255\n",
    "            for i, cl in enumerate(classes):\n",
    "                mapping[cl] = i\n",
    "            self.target_transform = lambda x: from_numpy(mapping[x])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is the label of segmentation.\n",
    "        \"\"\"\n",
    "\n",
    "        # # using read_image\n",
    "        img = read_image(os.path.join(self.root, \"images\", self.paths_images[index]))\n",
    "        target = read_image(os.path.join(self.root, \"labels\", self.paths_tagets[index]))\n",
    "\n",
    "        # if self.return_unprocessed_image:\n",
    "        #     transform_PIL = T.ToPILImage()\n",
    "        #     img = transform_PIL(img)\n",
    "        #     return img\n",
    "\n",
    "        if self.transform:\n",
    "            img, target = self.transform(img, target)\n",
    "\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target  # output: Tensor[image_channels, image_height, image_width]\n",
    "\n",
    "        # # using Image.open + np.array\n",
    "        # img = Image.open(os.path.join(self.root,\"images\",self.paths_images[index]))\n",
    "        # target = Image.open(os.path.join(self.root,\"labels\",self.paths_tagets[index]))\n",
    "\n",
    "        # return np.array(img), np.array(target) # output: Tensor[image_height, image_width, image_channels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = [\n",
    "        [128, 64, 128],\n",
    "        [244, 35, 232],\n",
    "        [70, 70, 70],\n",
    "        [102, 102, 156],\n",
    "        [190, 153, 153],\n",
    "        [153, 153, 153],\n",
    "        [250, 170, 30],\n",
    "        [220, 220, 0],\n",
    "        [107, 142, 35],\n",
    "        [152, 251, 152],\n",
    "        [0, 130, 180],\n",
    "        [220, 20, 60],\n",
    "        [255, 0, 0],\n",
    "        [0, 0, 142],\n",
    "        [0, 0, 70],\n",
    "        [0, 60, 100],\n",
    "        [0, 80, 100],\n",
    "        [0, 0, 230],\n",
    "        [119, 11, 32]    \n",
    "        ]\n",
    "\n",
    "label_colours = dict(zip(range(NUM_CLASSES), colors))\n",
    "\n",
    "def decode_segmap(temp):\n",
    "    #convert gray scale to color\n",
    "    #print colored map\n",
    "    temp=temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, NUM_CLASSES):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    return rgb\n",
    "\n",
    "def compute_moiu(net, val_dataloader):\n",
    "    net = net.to(DEVICE)\n",
    "    net.train(False)  # Set Network to evaluation mode\n",
    "    jaccard = MulticlassJaccardIndex(num_classes=NUM_CLASSES, ignore_index=255).to(\n",
    "        DEVICE\n",
    "    )\n",
    "\n",
    "    jacc = 0\n",
    "    count = 0\n",
    "    for images, labels in val_dataloader:\n",
    "        images = images.to(DEVICE, dtype=torch.float32)\n",
    "        labels = labels.to(DEVICE, dtype=torch.long)\n",
    "        # Forward Pass\n",
    "        outputs = net(images)\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update Corrects\n",
    "        jacc += jaccard(preds, labels.squeeze())\n",
    "        count += 1\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    metric = jacc.item() / count\n",
    "    # net.train(True)\n",
    "    return metric\n",
    "\n",
    "\n",
    "def validation_plot(net, val_dataloader, n_image):\n",
    "    net = net.to(DEVICE)\n",
    "    net.train(False)\n",
    "    rows = 1\n",
    "    columns = 3\n",
    "    for b, (imgs, targets) in enumerate(val_dataloader):\n",
    "        if b == n_image:\n",
    "            break\n",
    "        # i = random.randint(BATCH_SIZE)\n",
    "        imgsfloat = imgs.to(DEVICE, dtype=torch.float32)\n",
    "        outputs = net(imgsfloat)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        # Added in order to use the decode_segmap function\n",
    "        preds = preds.cpu()  # or equally preds = preds.to('cpu')\n",
    "\n",
    "        # pick the first image of each batch\n",
    "        print(imgs[0].shape, targets[0].shape)\n",
    "        print(\"img:\", imgs[0].squeeze().shape, \" target:\", targets[0].squeeze().shape)\n",
    "        print(\"pred:\", preds.shape)\n",
    "\n",
    "        figure = plt.figure(figsize=(10, 20))\n",
    "        figure.add_subplot(rows, columns, 1)\n",
    "        plt.imshow(imgs[0].permute((1, 2, 0)).squeeze())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Image\")\n",
    "\n",
    "        figure.add_subplot(rows, columns, 2)\n",
    "        plt.imshow(decode_segmap(targets[0].permute((1, 2, 0)).squeeze()))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Groundtruth\")\n",
    "\n",
    "        figure.add_subplot(rows, columns, 3)\n",
    "        plt.imshow(decode_segmap(preds[0].squeeze()))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting google drive and cloning FedDrive repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/AMLproject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./FedDrive'):\n",
    "  !git clone https://github.com/Erosinho13/FedDrive\n",
    "\n",
    "from FedDrive.src.modules.bisenetv2 import BiSeNetV2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 : GENERATING DATASETS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate splits for step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGES_FINAL = \"leftImg8bit\"\n",
    "TARGET_FINAL = \"gtFine_labelIds\"\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data/Cityscapes/train.txt\"), \"r\") as ft:\n",
    "    lines_train = ft.readlines()\n",
    "with open(os.path.join(ROOT_DIR, \"data/Cityscapes/val.txt\"), \"r\") as fv:\n",
    "    lines_val = fv.readlines()\n",
    "\n",
    "lines = lines_train + lines_val\n",
    "images = [\n",
    "    (\n",
    "        l.split(\"/\")[0],\n",
    "        l.strip().split(\"/\")[1],\n",
    "        l.strip().split(\"/\")[1].replace(IMAGES_FINAL, TARGET_FINAL),\n",
    "    )\n",
    "    for l in lines\n",
    "]\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data/Cityscapes/train_B.txt\"), \"w\") as f:\n",
    "    for l in lines_train:\n",
    "        img = l.strip().split(\"/\")[1]\n",
    "        lbl = img.replace(IMAGES_FINAL, TARGET_FINAL)\n",
    "        f.write(img + \"@\" + lbl + \"\\n\")\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data/Cityscapes/test_B.txt\"), \"w\") as f:\n",
    "    for l in lines_val:\n",
    "        img = l.strip().split(\"/\")[1]\n",
    "        lbl = img.replace(IMAGES_FINAL, TARGET_FINAL)\n",
    "        f.write(img + \"@\" + lbl + \"\\n\")\n",
    "\n",
    "city_dic = {}\n",
    "\n",
    "for i in images:\n",
    "    if i[0] not in city_dic:\n",
    "        city_dic[i[0]] = []\n",
    "        city_dic[i[0]].append(tuple(i[1:]))\n",
    "    else:\n",
    "        city_dic[i[0]].append(tuple(i[1:]))\n",
    "test_img = []\n",
    "train_img = []\n",
    "\n",
    "for c in city_dic.values():\n",
    "    s = random.sample(c, 2)\n",
    "    test_img += s\n",
    "    train_img += c\n",
    "    for img in s:\n",
    "        train_img.remove(img)\n",
    "\n",
    "# save the split\n",
    "with open(os.path.join(ROOT_DIR, \"data/Cityscapes/test_A.txt\"), \"w\") as f:\n",
    "    for img in test_img:\n",
    "        f.write(img[0] + \"@\" + img[1] + \"\\n\")\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"data/Cityscapes/train_A.txt\"), \"w\") as f:\n",
    "    for img in train_img:\n",
    "        f.write(str(img[0]) + \"@\" + img[1] + \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating splits for step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARTITION == \"A\":\n",
    "    # train A\n",
    "    with open(os.path.join(ROOT_DIR, \"data/Cityscapes/train_A.txt\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        images = [\n",
    "            (\n",
    "                l.strip().split(\"@\")[0],\n",
    "                l.strip().split(\"@\")[1],\n",
    "            )\n",
    "            for l in lines\n",
    "        ]\n",
    "if PARTITION == \"B\":\n",
    "    # train B\n",
    "    with open(os.path.join(ROOT_DIR, \"data/Cityscapes/train.txt\"), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        images = [\n",
    "            (\n",
    "                l.strip().split(\"/\")[1],\n",
    "                l.strip().split(\"/\")[1].replace(IMAGES_FINAL, TARGET_FINAL),\n",
    "            )\n",
    "            for l in lines\n",
    "        ]\n",
    "\n",
    "city_dic = {}\n",
    "for i in images:\n",
    "    city_name = i[0].split(\"_\")[0]\n",
    "    if city_name not in city_dic:\n",
    "        city_dic[city_name] = []\n",
    "    city_dic[city_name].append(i)\n",
    "\n",
    "if SPLIT == 1:\n",
    "    # uniform\n",
    "    # every client has images from different cityes\n",
    "    n_sample = len(images)\n",
    "    n_client_per_city = math.ceil(n_sample / MAX_SAMPLE_PER_CLIENT)\n",
    "    city_enum = list(enumerate(city_dic.keys()))\n",
    "    choices = [k for k in city_dic.keys()]\n",
    "    weights = [len(city_dic[c]) for c in choices]\n",
    "    client_dict = {}\n",
    "    for i in range(n_client_per_city):\n",
    "        client_dict[i] = []\n",
    "        for _ in range(MAX_SAMPLE_PER_CLIENT):\n",
    "            choices = [k for k in city_dic.keys()]\n",
    "            weights = [len(city_dic[c]) for c in choices]\n",
    "            try:\n",
    "                c = random.choices(choices, weights=weights, k=1)[0]\n",
    "            except:\n",
    "                break\n",
    "            img, lable = city_dic[c].pop()\n",
    "            client_dict[i].append((img, lable))\n",
    "            if len(city_dic[c]) == 0:\n",
    "                city_dic.pop(c)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(ROOT_DIR + \"data/Cityscapes/\", f\"uniform{PARTITION}.json\"), \"w\"\n",
    "    ) as outfile:\n",
    "        json.dump(client_dict, outfile, indent=4)\n",
    "\n",
    "if SPLIT == 2:\n",
    "    # heterogeneous\n",
    "    # every client has images from only one city\n",
    "    client_dict = {}\n",
    "    tot_clients = 0\n",
    "    for city in city_dic.keys():\n",
    "        n_samples_per_city = len(city_dic[city])\n",
    "        n_client_per_city = math.ceil(n_samples_per_city / MAX_SAMPLE_PER_CLIENT)\n",
    "        avg = len(city_dic[city]) // n_client_per_city\n",
    "\n",
    "        for i in range(tot_clients, tot_clients + n_client_per_city):\n",
    "            client_dict[i] = []\n",
    "            for _ in range(avg):\n",
    "                img, lbl = city_dic[city].pop()\n",
    "                client_dict[i].append((img, lbl))\n",
    "            tot_clients += 1\n",
    "        if len(city_dic[city]) > 0:\n",
    "            for img, lbl in city_dic[city]:\n",
    "                client_dict[i].append((img, lbl))\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            ROOT_DIR + \"data/Cityscapes/\", f\"heterogeneuos{PARTITION}.json\"\n",
    "        ),\n",
    "        \"w\",\n",
    "    ) as outfile:\n",
    "        json.dump(client_dict, outfile, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 : CENTRALIZED BASELINE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for WanDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "transformer_dictionary = {\n",
    "    \"random-horizontal-flip\":RANDOM_HORIZONTAL_FLIP,\n",
    "    \"color-jitter\":COLOR_JITTER,\n",
    "    \"random-rotation\":RANDOM_ROTATION,\n",
    "    \"random-crop\":RANDOM_CROP,\n",
    "    \"random-vertical-flip\":RANDOM_VERTICAL_FLIP,\n",
    "    \"central-crop\":CENTRAL_CROP,\n",
    "    \"random-resized-crop\":RANDOM_RESIZE_CROP,\n",
    "    \"resize\":RESIZE,\n",
    "    }\n",
    "\n",
    "config = {\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"step_size\": STEP_SIZE,\n",
    "        \"transformers\": transformer_dictionary,\n",
    "    }\n",
    "name = f\"Step_2_{PARTITION}_lr{LR}_bs{BATCH_SIZE}_e{NUM_EPOCHS}\"\n",
    "wandb.init(\n",
    "    project = \"AML-Project-2023\",\n",
    "    entity = \"aml-project2023\",\n",
    "    config = config,\n",
    "    name = name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = setup_transform()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if PARTITION == \"A\":\n",
    "    train_dataset = Cityscapes(\n",
    "        root=ROOT_DIR + \"data/Cityscapes/\",\n",
    "        transform=transforms,\n",
    "        cl19=True,\n",
    "        filename=\"train_A.txt\",\n",
    "    )\n",
    "elif PARTITION == \"B\":\n",
    "    train_dataset = Cityscapes(\n",
    "        root=ROOT_DIR + \"data/Citytscapes/\",\n",
    "        transform=transforms,\n",
    "        cl19=True,\n",
    "        filename=\"train_b.txt\",\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "parameters_to_optimize = model.parameters()\n",
    "optimizer = optim.SGD(\n",
    "    parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "cudnn.benchmark  # Calling this optimizes runtime\n",
    "\n",
    "epochs = []\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "current_step = 0\n",
    "# Start iterating over the epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Starting epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(DEVICE, dtype=torch.float32)\n",
    "        labels = labels.to(DEVICE, dtype=torch.long)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, labels.squeeze())\n",
    "\n",
    "        # Log loss\n",
    "        if current_step % LOG_FREQUENCY == 0:\n",
    "            print(\"Step {}, Loss {}\".format(current_step, loss.item()))\n",
    "            wandb.log({\"train/loss\": loss})\n",
    "        # Compute gradients for each layer and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"step2_{PARTITION}_model.pth\"\n",
    "if not os.path.exists(ROOT_DIR + \"models/STEP2/\"):\n",
    "    print(\"creating models directory\")\n",
    "    os.makedirs(ROOT_DIR + \"models/STEP2/\")\n",
    "torch.save(model.state_dict(), ROOT_DIR + \"models/STEP2/\" + name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Validation Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARTITION == \"A\":\n",
    "    val_dataset = Cityscapes(\n",
    "        root=ROOT_DIR + \"data/Cityscapes/\",\n",
    "        transform=transforms,\n",
    "        cl19=True,\n",
    "        filename=\"test_A.txt\",\n",
    "    )\n",
    "elif PARTITION == \"B\":\n",
    "    val_dataset = Cityscapes(\n",
    "        root=ROOT_DIR + \"data/Cityscapes/\",\n",
    "        transform=transforms,\n",
    "        cl19=True,\n",
    "        filename=\"test_B.txt\",\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"computing miou ...\")\n",
    "miou = compute_moiu(net=model, val_dataloader=val_dataloader)\n",
    "print(\"Validation MIoU: {}\".format(miou))\n",
    "wandb.log({\"val/miou\": miou})\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation plot : \")\n",
    "validation_plot(net=model, val_dataloader=val_dataloader, n_image=20)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 : FEDERATED + SEMANTIC SEGMENTATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class Client():\n",
    "  #def __init__(self, client_id, dataset, model, logger, writer, args, batch_size, world_size, rank, device=None, **kwargs):\n",
    "  def __init__(self, client_id, dataset, model):\n",
    "    self.id = client_id\n",
    "    self.dataset = dataset\n",
    "    self.model = model #copy.deepcopy(model)\n",
    "    self.device = DEVICE\n",
    "    self.batch_size = BATCH_SIZE\n",
    "    #self.args = args\n",
    "    self.loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "    # if args.random_seed is not None:\n",
    "    #     g = torch.Generator()\n",
    "    #     g.manual_seed(args.random_seed)\n",
    "    #     self.loader = data.DataLoader(self.dataset, batch_size=self.batch_size, worker_init_fn=seed_worker, num_workers=4, drop_last=True, pin_memory=True, generator=g)\n",
    "    # else:\n",
    "    #     self.loader = data.DataLoader(self.dataset, batch_size=self.batch_size, num_workers=4, drop_last=True, pin_memory=True)\n",
    "\n",
    "    #self.criterion = nn.CrossEntropyLoss(ignore_index=255, reduction='none')\n",
    "\n",
    "\n",
    "  def client_train(self):\n",
    "    \n",
    "    num_train_samples = len(self.dataset)\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index= 255)\n",
    "    parameters_to_optimize = self.model.parameters() \n",
    "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    self.model = self.model.to(DEVICE)\n",
    "    self.model.train() # Sets module in training mode\n",
    "\n",
    "    cudnn.benchmark # Calling this optimizes runtime\n",
    "    \n",
    "    # Start iterating over the epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "      if epoch % LOG_FREQUENCY_EPOCH == 0: \n",
    "        print('Starting epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
    "\n",
    "      # Iterate over the dataset\n",
    "      for current_step, (images, labels) in enumerate(self.loader):\n",
    "        images = images.to(DEVICE, dtype=torch.float32)\n",
    "        labels = labels.to(DEVICE, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = self.model(images)\n",
    "        loss = criterion(predictions, labels.squeeze())\n",
    "\n",
    "        # Log loss\n",
    "        if current_step % LOG_FREQUENCY == 0:\n",
    "          print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "          #wandb.log({f\"client{self.id}/loss\":loss})\n",
    "          wandb.log({f\"client/loss\":loss})\n",
    "\n",
    "        loss.backward()  # backward pass: computes gradients\n",
    "        optimizer.step() # update weights based on accumulated gradients\n",
    "\n",
    "    #return num_train_samples, copy.deepcopy(self.model.state_dict()) #generate_update\n",
    "    return num_train_samples, copy.deepcopy(self.model.state_dict()) #generate_update\n",
    "\n",
    "  def test(self, metrics, ret_samples_ids=None, silobn_type=None, train_cl_bn_stats=None, loader=None):\n",
    "    return\n",
    "\n",
    "  def save_model(self, epochs, path, optimizer, scheduler):\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Server():\n",
    "  #def __init__(self, model, logger, writer, local_rank, lr, momentum, optimizer=None):\n",
    "  def __init__(self, model, lr= None , momentum= None):\n",
    "    self.model = copy.deepcopy(model)\n",
    "    self.model_params_dict = copy.deepcopy(self.model.state_dict())\n",
    "    self.selected_clients = []\n",
    "    self.updates = []\n",
    "    self.lr = lr\n",
    "    self.momentum = momentum\n",
    "    self.optimizer = optim.SGD(params=self.model.parameters(), lr=1, momentum=0.9)\n",
    "    self.total_grad = 0\n",
    "\n",
    "  def select_clients(self, my_round, possible_clients, num_clients=4):\n",
    "    num_clients = min(num_clients, len(possible_clients))\n",
    "    np.random.seed(my_round)\n",
    "    self.selected_clients = np.random.choice(possible_clients, num_clients, replace=False)\n",
    "  \n",
    "  def _compute_client_delta(self, cmodel):\n",
    "      delta = OrderedDict.fromkeys(cmodel.keys())\n",
    "      for k, x, y in zip(self.model_params_dict.keys(), self.model_params_dict.values(), cmodel.values()):\n",
    "        #print(f'check is_cuda     y:{y.is_cuda} x:{x.is_cuda}')\n",
    "        delta[k] = y - x if \"running\" not in k and \"num_batches_tracked\" not in k else y\n",
    "      return delta\n",
    "  \n",
    "  def train_round(self):\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "\n",
    "    clients = self.selected_clients\n",
    "    losses = {}\n",
    "\n",
    "    for i, c in enumerate(clients):\n",
    "\n",
    "        print(f\"CLIENT {i + 1}/{len(clients)} -> {c.id}:\")\n",
    "\n",
    "        c.model.load_state_dict(self.model_params_dict) # load_server_model_on_client\n",
    "        out = c.client_train()\n",
    "        #c.save_bn_stats()\n",
    "\n",
    "        num_samples, update = out\n",
    "\n",
    "        update = self._compute_client_delta(update)\n",
    "        \n",
    "        self.updates.append((num_samples, update))\n",
    "    return \n",
    "\n",
    "  def _server_opt(self, pseudo_gradient):\n",
    "    for n, p in self.model.named_parameters():\n",
    "        p.grad = -1.0 * pseudo_gradient[n]\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    bn_layers = OrderedDict(\n",
    "        {k: v for k, v in pseudo_gradient.items() if \"running\" in k or \"num_batches_tracked\" in k})\n",
    "    self.model.load_state_dict(bn_layers, strict=False)\n",
    "\n",
    "  def _aggregation(self):\n",
    "    total_weight = 0.\n",
    "    base = OrderedDict()\n",
    "\n",
    "    for (client_samples, client_model) in self.updates:\n",
    "\n",
    "        total_weight += client_samples\n",
    "        for key, value in client_model.items():\n",
    "            if key in base:\n",
    "                base[key] += client_samples * value.type(torch.FloatTensor)\n",
    "            else:\n",
    "                base[key] = client_samples * value.type(torch.FloatTensor)\n",
    "    averaged_sol_n = copy.deepcopy(self.model_params_dict)\n",
    "    for key, value in base.items():\n",
    "        if total_weight != 0:\n",
    "            averaged_sol_n[key] = value.to('cuda') / total_weight\n",
    "\n",
    "    return averaged_sol_n\n",
    "\n",
    "  def _get_model_total_grad(self):\n",
    "    total_norm = 0\n",
    "    for name, p in self.model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_grad = total_norm ** 0.5\n",
    "    return total_grad\n",
    "\n",
    "  def update_model(self):\n",
    "    \"\"\"FedAvg on the clients' updates for the current round.\n",
    "    Weighted average of self.updates, where the weight is given by the number\n",
    "    of samples seen by the corresponding client at training time.\n",
    "    Saves the new central model in self.client_model and its state dictionary in self.model\n",
    "    \"\"\"\n",
    "\n",
    "    averaged_sol_n = self._aggregation()\n",
    "    \n",
    "    self._server_opt(averaged_sol_n)\n",
    "    self.total_grad = self._get_model_total_grad()\n",
    "    self.model_params_dict = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "    self.updates = []\n",
    "\n",
    "  def test_model(self, clients_to_test, metrics, ret_samples_bool=False, silobn_type=''):\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = setup_transform()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_clients(n_client, model):\n",
    "\n",
    "  clients = []\n",
    "  if PARTITION == 'A':\n",
    "    if SPLIT == 1:\n",
    "      filename=\"uniformA.json\"\n",
    "    else:\n",
    "      filename='heterogeneuosA.json'\n",
    "    for i in range(n_client):\n",
    "      train_dataset = Cityscapes(root=ROOT_DIR, transform=transforms, cl19 = cl19,filename=filename,  id_client = i)\n",
    "      client = Client(client_id = i, dataset = train_dataset, model = model)\n",
    "      clients.append(client)\n",
    "  else:\n",
    "    if SPLIT == 1:\n",
    "      filename=\"uniformB.json\"\n",
    "    else:\n",
    "      filename='heterogeneuosB.json'\n",
    "    for i in range(n_client):\n",
    "      train_dataset = Cityscapes(root=ROOT_DIR, transform=transforms, cl19 = cl19, filename=filename,  id_client = i)\n",
    "      client = Client(client_id = i, dataset = train_dataset, model = model)\n",
    "      clients.append(client)\n",
    "\n",
    "  return clients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clients_per_round = N_CLIENT\n",
    "model = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "wandb.watch(model, log='all')\n",
    "model_path = \"./models/Step3/\"\n",
    "#train_clients, test_clients = setup_clients(args, logger, writer, client_model, world_size, rank, args.n_devices, device, ckpt_path)\n",
    "train_clients = setup_clients(n_client = TOT_CLIENT, model = model)\n",
    "print(len(train_clients))\n",
    "#server = Server(model)#, lr = LR, momentum = MOMENTUM)\n",
    "server = Server(model, lr=LR, momentum = MOMENTUM)\n",
    "for r in range(N_ROUND):\n",
    "  print(f\"ROUND {r + 1}/{N_ROUND}: Training {CLIENT_PER_ROUND} Clients...\")\n",
    "  server.select_clients(r, train_clients, num_clients=CLIENT_PER_ROUND) \n",
    "  server.train_round()\n",
    "  server.update_model()\n",
    "  miou = compute_moiu(net=server.model, val_dataloader=val_dataloader)\n",
    "  wandb.log({\"server/miou\": miou})\n",
    "  print(f\"Validation MIoU: {miou}\")\n",
    "  if r%CHECKPOINTS == 0:\n",
    "    print(f\"Saving the model\")\n",
    "    torch.save(model.state_dict(), model_path+f\"model_P{PARTITION}_S{SPLIT}_round{r:02}.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), model_path+f\"model_P{PARTITION}_S{SPLIT}_round{r:02}.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"todo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"computing miou ...\")\n",
    "miou = compute_moiu(net=model, val_dataloader=val_dataloader)\n",
    "print(\"Validation MIoU: {}\".format(miou))\n",
    "wandb.log({\"val/miou\": miou})\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation plot : \")\n",
    "validation_plot(net=model, val_dataloader=val_dataloader, n_image=20)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
